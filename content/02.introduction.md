## Introduction

There are more academic papers than any human can read in a lifetime.
Solving the resulting prioritization problem and determining which papers are important is hard though.

"Importance" is a subjective measure though, and different metrics attempt to capture it through different approaches.
Citation count assumes the number of citations determines a paper's importance, h-index and journal impact factor focus more on secondary factors like author' or journals' track record, and graph-based methods like pagerank or disruption index use the context of the surrounding papers to evaluate a given article's relevance [CITE].
Each of these methods have their own strengths, as well as various permutations that shore up their weaknesses [CITE].

The purpose of this paper is not to create a new metric that attempts to measure articles' importance.
Instead, we aim to shed light on a less-studies aspect of prioritizing papers: differences between fields.
That there is a difference in papers' relevance in different fields is unsurprising.
Even Nobel prize-winning work will stay at the bottom of a cancer biologist's paper pile if the article is about astrophysics.

There are some metrics do account for differences between fields, such as the relative citation rate [CITE].
However, they tend to focus on normalizing out the differences to give an absolute ranking rather than quantifying how papers' importance differs across fields.

- We show that differences exist in the citation network pageranks

- We then demonstrate that some of the differences are in fact field-driven

- We also ... journals

- Finally, we built a web application to make our results more accessible.

- Going forward, we hope that field-aware methods of prioritizing papers appear.

