## Introduction

There are more academic papers than any human can read in a lifetime.
Solving the resulting prioritization problem and determining which papers are important is hard, however.

"Importance" is a subjective measure though, and different metrics attempt to capture it through different approaches.
Citation count assumes the number of citations determines a paper's importance, h-index and journal impact factor focus more on secondary factors like author' or journals' track record, and graph-based methods like pagerank or disruption index use the context of the surrounding papers to evaluate a given article's relevance [CITE].
Each of these methods have their own strengths, as well as various permutations that shore up their weaknesses [CITE].

- MeSH maps well to WoS categories https://www.issi-society.org/proceedings/issi_2009/ISSI2009-proc-vol2_Aug2009_batch2-paper-24.pdf

The purpose of this paper is not to create a new metric that attempts to measure articles' importance.
Instead, we aim to shed light on a less-studies aspect of prioritizing papers: differences between fields.
That there is a difference in papers' relevance in different fields is unsurprising.
Even Nobel prize-winning work will stay at the bottom of a cancer biologist's paper pile if the article is about astrophysics.
- Probably say something about how interdisciplinary papers can be important in one or both fields or origin

There are some metrics do account for differences between fields, such as the relative citation rate [CITE].
However, they tend to focus on normalizing out the differences to give an absolute ranking rather than quantifying how papers' importance differs across fields.

- Talk about journals
-- 
-- People have noted that journals aren't really fields before in https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5124055/ . They argue that journals are better for normalization, but don't actually use field-specific MeSH terms
-- Journals can be extremely or not at all influential depending on field: https://link.springer.com/article/10.1007/s11192-022-04402-w
-- https://www.scimagojr.com/ does journal rankings; uses PageRank just like Eigenfactor


In this paper we show differences between pageranks for the same paper in different field citation networks.
We then demonstrate that some of the differences are field-specific by estimating the distribution of possible citation networks via a degree-preserving graph shuffling method.
We also demonstrate that while it is possible to rank journals using standard metrics [CITE eigenfactor], their rank changes depending on field.
Finally, we make our results more easily available by creating a web app that visualizes pairs of fields with overlapping papers.

- Limitations of ranking papers
- Going forward, we hope that field-aware methods of prioritizing papers appear.
