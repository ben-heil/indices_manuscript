## Introduction

There are more academic papers than any human can read in a lifetime.
Solving the resulting prioritization problem and determining which papers are important is hard, however.

"Importance" is a subjective measure though, and different metrics attempt to capture it through different approaches.
Citation count assumes the number of citations determines a paper's importance, h-index and journal impact factor focus more on secondary factors like author' or journals' track record, and graph-based methods like pagerank or disruption index use the context of the surrounding papers to evaluate a given article's relevance [@doi:10.1073/pnas.0507655102; @jif; @pagerank; @doi:10.1038/s41586-019-0941-9].
Each of these methods have their own strengths, as well as various permutations that shore up their weaknesses [@doi:10.1371/journal.pbio.1002541; @doi:10.1016/j.joi.2010.01.002; @doi:10.1007/s11192-017-2626-1; @doi:10.1162/qss_a_00068].

Because there are differences between fields' citation practices [CITE], scientists have developed approaches like normalizing the number of citations based on nearby papers in the citation network, 
studying the relative performance of disruption index variants in a single field,
rescaling fields' citation data to give more consistant PageRank results, and so on [@doi:10.1371/journal.pbio.1002541; @doi:10.1007/s11192-020-03406-8; @doi:10.1016/j.joi.2017.05.014; @doi:10/f48tvt]
Such approaches account for differences between fields, but they tend to focus on normalizing out the effects of field on a metric.

We argue that this removal of field-specific signal obscures real differences in relevance.
Intuitively, Nobel prize-winning work will stay at the bottom of a cancer biologist's paper pile if the article is about astrophysics.
Trying to apply a universal ranking metric to works or journals ignores that phenomenon and yields strange results.
For example, Mason and Signh recently pointed out that depending on what field you're evaluating, the journal Christian Higher Education is either ranked as a Q1 (top tier) journal or a Q4 (bottom tier) journal [@doi:10.1007/s11192-022-04402-w].

The purpose of this paper is not to create a new metric that attempts to measure articles' importance.
Instead, we aim to shed light on a less-studies aspect of prioritizing papers: differences between fields.
In this paper we show differences between pageranks for the same paper in different field citation networks.
We then demonstrate that some of the differences are field-specific by estimating the distribution of possible citation networks via a degree-preserving graph shuffling method.
We also demonstrate that while it is possible to rank journals using standard metrics [@eigenfactor], their rank changes depending on field.
Finally, we make our results more easily available by creating a web app that visualizes pairs of fields with overlapping papers.

Ultimately we show that there is great potential in analyzing articles while keeping field in mind.
Going forward, we hope more field-aware methods of assessing article importance will be developed.
