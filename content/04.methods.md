## Methods

#### COCI
We used the March 2022 version of the COCI citation index [@doi:10.1007/s11192-019-03217-6] as the source of our citation data.
This dataset contains around 1.3 billion citations from ~73 million bibliographic resources.

#### Selecting fields
To differentiate between scientific fields, we needed a way to map papers to fields.
Fortunately, all the papers in Pubmed Central (https://www.ncbi.nlm.nih.gov/pmc/) have corresponding Medical Subject Headings (MeSH) terms.
While MeSH terms are varied and numerous, the subheadings of the Natural Science Disciplines (H01) category fit our needs.
However, MeSH terms are hierarchical and vary greatly in their size and specificity.
To extract a balanced set of terms, we recursively traversed the tree and selected headings having at least 10000 DOIs without having multiple children that also meet the cutoff.
Our resulting headings were comprised of 45 terms, from "Acoustics" to "Water Microbiology."

#### Building single heading citation networks
The COCI dataset consists of pairs of Digital Object Identifiers (DOIs).
To change these pairs into a form we could run calculations on, we needed to convert them into networks.
To do so, we created 45 empty networks, one for each previously selected MeSH term.
We then iterated over each pair of DOIs in COCI and added them to a network if the DOIs corresponded to two journal articles written in English, both of which were tagged with the corresponding MeSH heading.


Because we were interested in the differences between fields, we also needed to build networks from pairs of MeSH headings.
These networks were built via the same process, except that instead of keeping articles corresponding to a single DOI we added a citation to the network if both articles were in the pair of fields, even if the citation occurred across fields.
Running this network-building process yielded 990 two-heading networks.

Sampling a graph from the degree distribution while preserving the distribution of degrees in the network was challenging.
Because citation graphs are directed, it is impossible to simply swap pairs of edges and end up with a graph uniformly sampled from the space.
Instead, a more sophisticated three-edge swap method must be used [@arxiv:0905.4913].
Because this algorithm had not been implemented yet in NetworkX [@networkx], we implemented the code to perform shuffles and submitted our change to the library. <!-- DO YOU WANT TO CITE THE PULL REQUEST HERE VIA URL? -->
With the shuffling code implemented, we created 100 shuffled versions of each of our combined networks to act as a background distribution against which we could compare metrics.

Once we had a collection of shuffled networks, we needed to split them into their constituent fields.
To do so, we reduced the network to solely the nodes that were present in the single heading citation network and kept only citations between these nodes.

#### Metrics

We used the NetworkX implementation of PageRank with default parameters to evaluate paper importance within fields.
To determine the degree to which the papers' PageRank values were higher or lower than expected, we compared the PageRank values calculated for the true citation networks to the values in the shuffled networks for each paper.
We then recorded the percent of shuffled networks where the paper had a lower PageRank than the true network to derive a single number that described these values.
For example, if a paper had a higher PageRank in the true network than in all the shuffled networks it received a percentile of 100.
Likewise, if it had a lower PageRank in the true network than in all the shuffled networks it received a percentile of 0.


A convenient feature of the percentiles was that they were directly comparable between fields.
For manuscripts represented in two fields, the difference in scores was used to estimate its variability in importance.
For example, if a paper had a score of 100 in field A (indicating a higher PageRank in the field than expected given its number of citations and the network structure) and a score of 0 in field B (indicating a lower than expected PageRank), then the large difference in scores indicated the paper was more highly valued in field A than field B.
If the paper had similar scores in both fields, it indicated that the paper was similarly valued in the two fields.

#### Hardware/runtime

We ran the full analysis pipeline on the RMACC Summit cluster at the University of Colorado.
The pipeline took about a week to run, from downloading the data to analyzing it to visualizing it.
Performance in other contexts will depend heavily on details such as the number of CPU nodes available and the network speed.

#### Server details

Our webserver is built by visualizing our data in Plotly (https://plotly.com/python/plotly-express/) on the Streamlit platform (https://streamlit.io/).
The field pairs made available by the frontend are those with at least 1000 shared papers after filtering out papers with more than a 5% missingness level of their PageRanks after shuffling.
The journals available for visualization are those with at least 25 papers for the given field pair.
