## Discussion/Conclusion

We analyzed hundreds of pairwise citation networks in order to determine the effects of field on citation metrics.
In doing so we found that there are, in fact, differences in pageranks between fields, that warrant some form of normalization when making comparison.
However, we also showed that these differences aren't solely differences in citation practices that should be regressed out.
Instead, there appear to be meaningful differences between fields for individual papers that exist in both fields.
Normalizing out this variation or create a single global ranking obscures meaningful signal about how papers are perceived in different communities.

That is not to say that our analysis is perfect, however.
For one thing, there are inherent issues with the concept of ranking papers.
While we point out that not using field information in prioritizing papers can lose some of the nuance of the data, we're still just discussing different ways of measuring proxy variables.
One's goal is often to read the papers most relevant to them, or the most influential papers in their field.
However, since the ground truth of relevance or influence is difficult or impossible to quanitfy we end up using PageRank, citation count, Journal Impact Factor, or some other metric.
While these proxy variables are hopefully correlated with the characteristics we care about, its important to acknowledge that theyr'e not the same thing.

Our percentile score is also an area for potential improvement.
We decide to make three times as many swaps as our network has edges, but it is unclear what the ideal number of swaps is to ensure a network is drawn uniformly at random from the distribution of possible networks with the same degree distribution.
Additionally, sampling graphs at random from degree-preserving network swaps isn't perfectly representative of what a potential alternate universe's citation network would look like.
For that we'd need a way to avoid edge swaps that caused "back-in-time" citations, a method to swap citations to a paper based on what similar papers had cited, and the ability to add or subtract citations to and from a given paper, as the number of citations for a given paper is partly due to random chance.
While outside of the scope of this paper, implementing these features into a citation graph randomization method would be an interesting avenue of future research.

A third limitation of our paper comes from the way we selected MeSH terms.
Our threshold number of papers required to include a field was chosen so that we could investigate a diverse set of fields while still keeping a good amount of papers in each combination of fields.
However, it is possible that a more (or less) granular field selection approach would give clearer results.

Similar approaches to ours may also be helpful for an expert in one field learning a new one.
For example, a expert in computational biology might get a better idea of the difference in perspective across fields by reading articles that have a high percentile score in genetics and a low score in computational biology.
While such papers are important in genetics, the computational biologist would be unlikely to be familiar with them due to the field difference.

Ultimately we hope to see more analysis going forward use field information for evaluating the influence of scientific articles.
We believe this to be particularly important in the case of interdisciplinary papers that exist between fields by their very nature.
A paper largely ignored in one field may be groundbreaking in another, and that dynamic should be accounted for when attempting to make sense of citation networks.
